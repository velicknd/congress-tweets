---
title: "Congress Tweets"
output: html_document
date: '2022-11-10'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import Packages
```{r}
libs = c('jsonlite', 'tidyjson', 'ggplot2', 'tidyverse','knitr', 'purrr', 'rvest', 'git2r', 'fs', 'tictoc','tidytext','tm','dtplyr')
invisible(lapply(libs, library, character.only = TRUE))
```

# Load Github data into RDS file
```{r}
# # Single day example ----
# # url <- 'https://alexlitel.github.io/congresstweets/data/2022-11-09.json'
# # today_example <- fromJSON(txt=url)
# 
# # Clone Repository ----
# tic()
# temp_import_dir <- path_temp("githubRepo")
# repo_url <- "https://github.com/alexlitel/congresstweets.git"
# clone(url = repo_url, local_path = temp_import_dir)
# toc()
# 
# # Getting the actual files ----
# #get file names as a list
# files_list <- dir_ls(path = path(temp_import_dir, "data"),
#                      glob = "*.json")
# #parse JSON files into R dataframes, bind them all together
# data_files <- map_df(files_list, fromJSON)
# saveRDS(data_files, "data_files.rds")
```

# Load RDS file
```{r}
congress_tweets <- readRDS("data_files.rds")
```


# Clean up data
```{r}
congress_tweets_clean <- congress_tweets %>%
  # head(100000)%>%
  mutate(date_time = lubridate::as_datetime(str_sub(str_replace(time, "T", " "), 0, 19)),
         year = lubridate::year(date_time))%>%
  select(id, screen_name, date_time, year, text, source, user_id, link)

# congress_tweets_clean <- data.table::as.data.table(congress_tweets_clean)

#mutate num words per tweet

```

# 1. Whatâ€™s the Twitter usage for the users (aka members of Congress) between start and end of the dataset?
```{r}
# Start: June 21, 2017
# End: Nov. 9, 2022

#groupby and tally by year


```


# 2. What are the top 20 words that are used in their tweets? What do they tell us about the data?
```{r}
#dataframe of English stopwords
en_stopwords = data.frame(word = stopwords("en"))

#remove URLs
tic()
congress_tweets_clean$stripped_text <- gsub("http.*","",  congress_tweets_clean$text)
congress_tweets_clean$stripped_text <- gsub("https.*","", congress_tweets_clean$stripped_text)
toc()

#calculate word frequency
tic()
frequency_dataframe17 <- congress_tweets_clean%>%
  filter(year == 2017)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe17, "frequency_dataframe17.rds")

tic()
frequency_dataframe18 <- congress_tweets_clean%>%
  filter(year == 2018)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe18, "frequency_dataframe18.rds")

# tic()
# frequency_dataframe19 <- congress_tweets_clean%>%
#   filter(year == 2019)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe19, "frequency_dataframe19.rds"),
# 
# tic()
# frequency_dataframe20 <- congress_tweets_clean%>%
#   filter(year == 2020)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe20, "frequency_dataframe20.rds")
# 
# tic()
# frequency_dataframe21 <- congress_tweets_clean%>%
#   filter(year == 2021)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe21, "frequency_dataframe21.rds")

tic()
frequency_dataframe22 <- congress_tweets_clean%>%
  filter(year == 2022)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe22, "frequency_dataframe22.rds")
#ERROR when run on all tweets

# get_word_freq <- function(df, year) {
#   df_year <- df %>% filter(year == year)
#   
#   freq_year <- df_year %>%
#     select(stripped_text) %>%
#     na.omit() %>%
#     unnest_tokens(output = word, input = stripped_text, token = "tweets", to_lower = TRUE) %>%
#     anti_join(en_stopwords) %>% #get rid of stop words
#     count(word) %>%
#     arrange(desc(n))
#   
#   return(freq_year)
# }
# tic()
# word_freq_2017 <- get_word_freq(congress_tweets_clean, 2017)
# toc()
# tic()
# word_freq_2018 <- get_word_freq(congress_tweets_clean, 2018)
# toc()
# tic()
# word_freq_2019 <- get_word_freq(congress_tweets_clean, 2019)
# toc()
# tic()
# word_freq_2020 <- get_word_freq(congress_tweets_clean, 2020)
# toc()
# tic()
# word_freq_2021 <- get_word_freq(congress_tweets_clean, 2021)
# toc()
# tic()
# word_freq_2022 <- get_word_freq(congress_tweets_clean, 2022)
# toc()


```

# 3. Who are the power users?
```{r}
congress_tweets_by_user <- congress_tweets_clean %>%
  group_by(screen_name, user_id)%>%
  tally()%>%
  arrange(desc(n))

congress_tweets_by_user_year <- congress_tweets_clean %>%
  group_by(screen_name, user_id, year)%>%
  tally()%>%
  arrange(desc(n))

#mutate avg num words per tweet

```

# 4.  feel free to add in any other interesting takeaways from the data that you think are worth sharing!
```{r}
#who mentions Donald Trump the most in tweets? over what period of time?



```



