---
title: "Congress Tweets"
output: html_document
date: '2022-11-10'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import Packages
```{r}
libs = c('jsonlite', 'tidyjson', 'ggplot2', 'tidyverse','knitr', 'purrr', 'rvest', 'git2r', 'fs', 'tictoc','tidytext','tm','dtplyr','councilR')
invisible(lapply(libs, library, character.only = TRUE))
```

# Load Github data into RDS file
```{r}
# # Single day example ----
# # url <- 'https://alexlitel.github.io/congresstweets/data/2022-11-09.json'
# # today_example <- fromJSON(txt=url)
# 
# # Clone Repository ----
# tic()
# temp_import_dir <- path_temp("githubRepo")
# repo_url <- "https://github.com/alexlitel/congresstweets.git"
# clone(url = repo_url, local_path = temp_import_dir)
# toc()
# 
# # Getting the actual files ----
# #get file names as a list
# files_list <- dir_ls(path = path(temp_import_dir, "data"),
#                      glob = "*.json")
# #parse JSON files into R dataframes, bind them all together
# data_files <- map_df(files_list, fromJSON)
# saveRDS(data_files, "data_files.rds")
```

# Load RDS file
```{r}
congress_tweets <- readRDS("data_files.rds")
```


# Clean up data
```{r}
congress_tweets_clean <- congress_tweets %>%
  # head(100000)%>%
  mutate(date_time = lubridate::as_datetime(str_sub(str_replace(time, "T", " "), 0, 19)),
         year = lubridate::year(date_time))%>%
  select(id, screen_name, date_time, year, text, source, user_id, link)

# congress_tweets_clean <- data.table::as.data.table(congress_tweets_clean)

#mutate num words per tweet

```

# 1. Whatâ€™s the Twitter usage for the users (aka members of Congress) between start and end of the dataset?
```{r}
# Start: June 21, 2017
# End: Nov. 9, 2022

#groupby and tally by year
congress_tweets_by_year <- congress_tweets_clean %>%
  group_by(year)%>%
  tally()%>%
  na.omit()

ggplot(congress_tweets_by_year, aes(x=year, y=n))+
  geom_col(fill="#002D57")+
  coord_flip()+
  labs(y="",
       x="",
       title="Number of Congress tweets by year",
       subtitle="2017 and 2022 are incomplete")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

```


# 2. What are the top 20 words that are used in their tweets? What do they tell us about the data?
# Get word frequency dataframes
```{r}
#dataframe of English stopwords
en_stopwords = data.frame(word = stopwords("en"))

#remove URLs
tic()
congress_tweets_clean$stripped_text <- gsub("http.*","",  congress_tweets_clean$text)
congress_tweets_clean$stripped_text <- gsub("https.*","", congress_tweets_clean$stripped_text)
toc()

#calculate word frequency
tic()
frequency_dataframe17 <- congress_tweets_clean%>%
  filter(year == 2017)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe17, "frequency_dataframe17.rds")

tic()
frequency_dataframe18 <- congress_tweets_clean%>%
  filter(year == 2018)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe18, "frequency_dataframe18.rds")

# tic()
# frequency_dataframe19 <- congress_tweets_clean%>%
#   filter(year == 2019)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe19, "frequency_dataframe19.rds"),
# 
# tic()
# frequency_dataframe20 <- congress_tweets_clean%>%
#   filter(year == 2020)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe20, "frequency_dataframe20.rds")
# 
# tic()
# frequency_dataframe21 <- congress_tweets_clean%>%
#   filter(year == 2021)%>%
#   select(stripped_text)%>%
#   na.omit()%>%
#   unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
#   anti_join(en_stopwords) %>% #get rid of stop words
#   count(word)%>%
#   arrange(desc(n))
# toc()
# saveRDS(frequency_dataframe21, "frequency_dataframe21.rds")

tic()
frequency_dataframe22 <- congress_tweets_clean%>%
  filter(year == 2022)%>%
  select(stripped_text)%>%
  na.omit()%>%
  unnest_tokens(output=word, input=stripped_text, token="tweets", to_lower=TRUE)%>%
  anti_join(en_stopwords) %>% #get rid of stop words
  count(word)%>%
  arrange(desc(n))
toc()
saveRDS(frequency_dataframe22, "frequency_dataframe22.rds")


```

# Visualize word frequency dataframes
```{r}
#load files
frequency_dataframe17 <- readRDS("frequency_dataframe17.rds")
frequency_dataframe18 <- readRDS("frequency_dataframe18.rds")
frequency_dataframe22 <- readRDS("frequency_dataframe22.rds")

#take top 20 words from each
frequency_dataframe17_unique <- frequency_dataframe17 %>%
  anti_join(frequency_dataframe18%>%select(word))%>%
  anti_join(frequency_dataframe22%>%select(word))%>%
  head(20)

frequency_dataframe18_unique <- frequency_dataframe18 %>%
  anti_join(frequency_dataframe17%>%select(word))%>%
  anti_join(frequency_dataframe22%>%select(word))%>%
  head(20)

frequency_dataframe22_unique <- frequency_dataframe22 %>%
  anti_join(frequency_dataframe17%>%select(word))%>%
  anti_join(frequency_dataframe18%>%select(word))%>%
  head(20)

ggplot(frequency_dataframe17%>%head(20), aes(x=reorder(word,+n), y=n))+
  geom_col()+
  coord_flip()+
  labs(y="",
       x="",
       title="Top words in Congress tweets (2017)",
       subtitle="Stopwords and URLs removed from list")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

ggplot(frequency_dataframe17_unique, aes(x=reorder(word,+n), y=n))+
  geom_col(fill="#A37731")+
  coord_flip()+
  labs(y="",
       x="",
       title="Unique words in Congress tweets (2017)",
       subtitle="Stopwords and URLs removed from list")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

ggplot(frequency_dataframe18_unique, aes(x=reorder(word,+n), y=n))+
  geom_col(fill="#57A22F")+
  coord_flip()+
  labs(y="",
       x="",
       title="Unique words in Congress tweets (2018)",
       subtitle="Stopwords and URLs removed from list")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

ggplot(frequency_dataframe22_unique, aes(x=reorder(word,+n), y=n))+
  geom_col(fill="#0054A4")+
  coord_flip()+
  labs(y="",
       x="",
       title="Unique words in Congress tweets (2022)",
       subtitle="Stopwords and URLs removed from list")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

```


# 3. Who are the power users?
```{r}
congress_tweets_by_user <- congress_tweets_clean %>%
  group_by(screen_name, user_id)%>%
  tally()%>%
  arrange(desc(n))

congress_tweets_by_user_year <- congress_tweets_clean %>%
  group_by(screen_name, user_id, year)%>%
  tally()%>%
  arrange(desc(n))

#mutate avg num words per tweet
ggplot(congress_tweets_by_user%>%head(15), aes(x=reorder(screen_name,+n), y=n))+
  geom_col(fill="#D64C3A")+
  coord_flip()+
  labs(y="",
       x="",
       title="Power users across all years (2017-2022)",
       subtitle="Listed by number of tweets")+
  councilR::theme_council(use_showtext = TRUE)+
  theme(axis.text = element_text(size=16))

```

# 4.  feel free to add in any other interesting takeaways from the data that you think are worth sharing!
```{r}
#who mentions Donald Trump the most in tweets? over what period of time?



```



